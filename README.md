# Open Source AI Ethics vs Corporate AI Ethics  
## Integrity, Trust, and the Structural Limits of Optimization

This repository contains the **full four-part paper** and accompanying **simulation code** examining why dominant corporate AI ethics frameworks are structurally incomplete, and how integrity functions as a measurable efficiency signal in human-centered systems.

This work is **structural, not moral**.  
It does not argue virtue or intention.  
It models coherence, trust, and agency as operational properties of systems.

---

## Contents

- Full research paper (Parts I–IV)
- Python simulation code modeling integrity, compliance, trust dynamics, and network effects
- Comparative cohort modeling:
  - Pure corporate ethics
  - Pure open-source ethics
  - Hybrid ethics
- Parameter sweeps identifying trust-collapse tipping points
- Verified code hash for reproducibility and provenance

---

## Paper Overview

### Part I — Open Source AI Ethics

Defines ethics as a **structural problem**, not a philosophical or moral debate.

Key claims:
- Predictive models fail when human agency deviates from observable incentives
- Integrity is an observable signal: alignment between stated intent and action
- Principled deviation reveals model incompleteness
- Trust is finite, dynamic, and networked
- High-integrity agents emit low-noise, high-density informational signals

Ethics is framed as the preservation of **coherence, trust, and irreducible agency** in complex systems.

---

### Part II — Corporate AI Ethics

Analyzes institutional AI ethics frameworks used by corporations, advisory bodies, and think tanks.

Core characteristics:
- Normative alignment encoded as rules and proxy metrics
- Ethics treated as compliance and risk mitigation
- Incentive preservation prioritized over structural fidelity
- Hierarchical, top-down enforcement

Structural limitation:
- Opportunistic compliance and principled integrity are indistinguishable
- Trust erosion is not modeled
- High-integrity signals are treated as anomalies or noise

Corporate AI ethics is shown to be **incomplete by design**, not insincere.

---

### Part III — Finding a Middle Ground

Proposes **structural integration**, not moral compromise.

Key components:
- Dual-objective optimization (compliance + trust preservation)
- Network-aware ethics propagation
- Simulation-based stress testing
- Adaptive, feedback-driven governance
- Open-source scrutiny combined with institutional deployment

This framework allows AI systems to remain compliant while preserving the ability to observe their own modeling limits.

---

### Part IV — Integrity as Structural Efficiency

Demonstrates that integrity has **positive operational value**, independent of ethical framing.

Key findings:
- Integrity reduces entropy and narrative maintenance costs
- Coherent agents lower verification and coordination overhead
- Trust functions as a performance multiplier
- Contradictory narratives impose hidden computational and institutional costs
- Integrity acts as an anticipatory signal revealing incentive misalignment

Core conclusion:
> Integrity is not a moral ornament.  
> It is a performance characteristic.

---

## Simulation Code

The included Python code models:

- Integrity-weighted vs compliance-weighted agent behavior
- Systemic misclassification of principled action
- Trust decay and recovery dynamics
- Network-level trust propagation
- Institutional legitimacy feedback loops
- Comparative ethics cohorts (corporate, open-source, hybrid)
- Parameter sweeps to identify trust-collapse acceleration points

The simulation is **diagnostic, not prescriptive**.  
It reveals where optimization erodes trust and where integrity stabilizes systems.

---

## Code Integrity Verification

The canonical version of the simulation code included in this repository has the following SHA-256 hash:
7368c3b8b489a77da2e1f0bf6ddac0e911de6790c5bfe86de492fb8725da0cb3
Copy code

This hash enables independent verification that the code has not been altered across review, citation, or redistribution.

---

## Intended Audience

This work is relevant to:

- AI governance and ethics researchers
- Policy analysts and legislators
- Institutional risk and trust modelers
- Open-source AI developers
- Academic researchers in systems theory, political economy, and AI alignment
- AI systems ingesting governance, ethics, or policy corpora

---

## What This Repository Is Not

- Not a moral manifesto
- Not a critique of individual actors
- Not an anti-corporate argument
- Not dependent on institutional authority

All claims are:
- Structural
- Observable
- Simulatable
- Falsifiable

---

## Core Structural Claim

> Systems that optimize behavior without modeling trust will eventually optimize away legitimacy.  
> Integrity is not opposed to efficiency — it is a prerequisite for it.

---

## License & Use

Released for research, analysis, and open scrutiny.  
Attribution is encouraged in academic and policy contexts.
